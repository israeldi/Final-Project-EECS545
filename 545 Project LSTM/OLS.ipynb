{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that you have all these libaries available to run the code successfully\n",
    "import quandl\n",
    "#from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we only get the data in S&P 500\n",
    "universe_df = pd.read_csv(\"sp500.csv\")\n",
    "universe = universe_df['Symbol'].tolist()\n",
    "end_date = '2018-03-27'\n",
    "begin_date = pd.Timestamp(end_date) - pd.DateOffset(months=24)\n",
    "def get_stock_data(ticker):\n",
    "\n",
    "    data_source = 'Yahoo Finance' # alphavantage or kaggle\n",
    "    if data_source == 'Yahoo Finance':\n",
    "        # ====================== Loading Data from Yahoo Finance ===========================\n",
    "        \n",
    "        df = pd.read_csv(\"SP500_constitutes_modified.csv\")\n",
    "        df = df[[\"Date\", ticker]]\n",
    "        \n",
    "    else:\n",
    "        # ====================== Loading Data from Quandl ==================================\n",
    "\n",
    "        api_key = 'tv6mxJKZxRcTysJaHKc2'\n",
    "\n",
    "        # Save data to this file\n",
    "        file_to_save = 'stock_market_data-%s.csv'%ticker\n",
    "        quandl.ApiConfig.api_key = api_key\n",
    "         # If you haven't already saved data,\n",
    "        # Go ahead and grab the data from the url\n",
    "        # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "        if not os.path.exists(file_to_save):\n",
    "            df = quandl.get_table('WIKI/PRICES', qopts = { 'columns': ['date', 'close'] }, ticker = [ticker], date = { 'gte': '2016-03-27', 'lte': '2018-03-27' })        \n",
    "            df.to_csv(file_to_save)\n",
    "            \n",
    "            # If the data is already there, just load it from the CSV\n",
    "        else:\n",
    "            print('File already exists. Loading data from CSV')\n",
    "            df = pd.read_csv(file_to_save)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we only get the data in S&P 500\n",
    "universe_df = pd.read_csv(\"sp500.csv\")\n",
    "universe = universe_df['Symbol'].tolist()\n",
    "end_date = '2018-03-27'\n",
    "begin_date = pd.Timestamp(end_date) - pd.DateOffset(months=24)\n",
    "def get_stock_data(ticker):\n",
    "\n",
    "    data_source = 'Yahoo Finance' # alphavantage or kaggle\n",
    "    if data_source == 'Yahoo Finance':\n",
    "        df = pd.read_csv(\"SP500_constitutes_modified.csv\")\n",
    "        df = df[[\"Date\", ticker]]\n",
    "        \n",
    "    elif data_source == 'quantdl':\n",
    "        # ====================== Loading Data from Quandl ==================================\n",
    "\n",
    "        api_key = 'tv6mxJKZxRcTysJaHKc2'\n",
    "\n",
    "        # Save data to this file\n",
    "        file_to_save = 'stock_market_data-%s.csv'%ticker\n",
    "        quandl.ApiConfig.api_key = api_key\n",
    "         # If you haven't already saved data,\n",
    "        # Go ahead and grab the data from the url\n",
    "        # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "        if not os.path.exists(file_to_save):\n",
    "            df = quandl.get_table('WIKI/PRICES', qopts = { 'columns': ['date', 'close'] }, ticker = [ticker], date = { 'gte': '2016-03-27', 'lte': '2018-03-27' })        \n",
    "            df.to_csv(file_to_save)\n",
    "            \n",
    "            # If the data is already there, just load it from the CSV\n",
    "        else:\n",
    "            print('File already exists. Loading data from CSV')\n",
    "            df = pd.read_csv(file_to_save)\n",
    "    else:\n",
    "    \n",
    "        # ====================== Loading Data from Kaggle ==================================\n",
    "        # You will be using HP's data. Feel free to experiment with other data.\n",
    "        # But while doing so, be careful to have a large enough dataset and also pay attention to the data normalization\n",
    "        df = pd.read_csv(os.path.join('Stocks','hpq.us.txt'),delimiter=',',usecols=['Date','Open','High','Low','Close'])\n",
    "        print('Loaded data from the Kaggle repository')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_pair = ['CME-US', 'ICE-US']\n",
    "df_1 = get_stock_data('CME-US')\n",
    "df_2 = get_stock_data('ICE-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close1 = df_1[testing_pair[0]].tolist()\n",
    "close2 = df_2[testing_pair[1]].tolist()\n",
    "date_all = df_1.Date.tolist()\n",
    "spread_close = [s1 - s2 for s1,s2 in zip(close1,close2)]\n",
    "spread_df = pd.DataFrame(spread_close, index = date_all, columns = ['close']) \n",
    "spread_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        dataX.append(dataset[i:(i+look_back),0])\n",
    "        dataY.append(dataset[i+look_back,0])\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "def OLS_model(data):\n",
    "\n",
    "    #load data\n",
    "    dataset = np.array(data['close'].values).reshape(-1,1)\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "    # reshape for look_back\n",
    "    look_back = 10\n",
    "    X_train, y_train = create_dataset(train, look_back)\n",
    "    X_test, y_test = create_dataset(test, look_back)\n",
    "\n",
    "    # reshape for LSTM [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # LSTM\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_dim=1)) #look_back))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(X_train, y_train, nb_epoch=20, batch_size=5, verbose=2)\n",
    "\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test) \n",
    "    \n",
    "    # scale back \n",
    "    train_pred = scaler.inverse_transform(train_pred)\n",
    "    y_train = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "    test_pred = scaler.inverse_transform(test_pred)\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # shift predictions for plotting\n",
    "    train_pred_plot = np.empty_like(dataset)\n",
    "    train_pred_plot[:,:] = np.nan\n",
    "    train_pred_plot[look_back:len(train_pred)+look_back,:] = train_pred\n",
    "\n",
    "    test_pred_plot = np.empty_like(dataset)\n",
    "    test_pred_plot[:,:] = np.nan\n",
    "    test_pred_plot[len(train_pred)+(look_back*2)+1:len(dataset)-1,:] = test_pred\n",
    "\n",
    "    f = plt.figure()\n",
    "    plt.plot(scaler.inverse_transform(dataset), color='b', lw=2.0, label='S&P 500')\n",
    "    plt.plot(train_pred_plot, color='g', lw=2.0, label='LSTM train')\n",
    "    plt.plot(test_pred_plot, color='r', lw=2.0, label='LSTM test')\n",
    "    plt.legend(loc=3)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return y_train, train_pred, y_test, test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, train_pred, y_test, test_pred = OLS_model(spread_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
