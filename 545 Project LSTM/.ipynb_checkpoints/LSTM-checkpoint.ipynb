{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-e85928202d31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Make sure that you have all these libaries available to run the code successfully\n",
    "import quandl\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we only get the data in S&P 500\n",
    "universe_df = pd.read_csv(\"sp500.csv\")\n",
    "universe = universe_df['Symbol'].tolist()\n",
    "end_date = '2018-03-27'\n",
    "begin_date = pd.Timestamp(end_date) - pd.DateOffset(months=24),\n",
    "def get_stock_data(ticker):\n",
    "\n",
    "    data_source = 'quantdl' # alphavantage or kaggle\n",
    "    \n",
    "    if data_source == 'alphavantage':\n",
    "        # ====================== Loading Data from Alpha Vantage ==================================\n",
    "\n",
    "        api_key = 'MZ7WPF2UQLBYADGY'\n",
    "\n",
    "        # JSON file with all the stock market data for AAL from the last 20 years\n",
    "        url_string = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=%s&outputsize=full&apikey=%s\"%(ticker,api_key)\n",
    "\n",
    "        # Save data to this file\n",
    "        file_to_save = 'stock_market_data-%s.csv'%ticker\n",
    "\n",
    "        # If you haven't already saved data,\n",
    "        # Go ahead and grab the data from the url\n",
    "        # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "        if not os.path.exists(file_to_save):\n",
    "            with urllib.request.urlopen(url_string) as url:\n",
    "                data = json.loads(url.read().decode())\n",
    "                # extract stock market data\n",
    "                data = data['Time Series (Daily)']\n",
    "                df = pd.DataFrame(columns=['Date','Low','High','Close','Open'])\n",
    "                for k,v in data.items():\n",
    "                    date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
    "                    data_row = [date.date(),float(v['3. low']),float(v['2. high']),\n",
    "                                float(v['4. close']),float(v['1. open'])]\n",
    "                    df.loc[-1,:] = data_row\n",
    "                    df.index = df.index + 1\n",
    "            print('Data saved to : %s'%file_to_save)        \n",
    "            df.to_csv(file_to_save)\n",
    "\n",
    "        # If the data is already there, just load it from the CSV\n",
    "        else:\n",
    "            print('File already exists. Loading data from CSV')\n",
    "            df = pd.read_csv(file_to_save)\n",
    "    elif data_source == 'quantdl':\n",
    "        # ====================== Loading Data from Quandl ==================================\n",
    "\n",
    "        api_key = 'tv6mxJKZxRcTysJaHKc2'\n",
    "\n",
    "        # Save data to this file\n",
    "        file_to_save = 'stock_market_data-%s.csv'%ticker\n",
    "        quandl.ApiConfig.api_key = api_key\n",
    "         # If you haven't already saved data,\n",
    "        # Go ahead and grab the data from the url\n",
    "        # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "        if not os.path.exists(file_to_save):\n",
    "            df = quandl.get_table('WIKI/PRICES', qopts = { 'columns': ['date', 'close'] }, ticker = [ticker], date = { 'gte': '2016-03-27', 'lte': '2018-03-27' })        \n",
    "            df.to_csv(file_to_save)\n",
    "            \n",
    "            # If the data is already there, just load it from the CSV\n",
    "        else:\n",
    "            print('File already exists. Loading data from CSV')\n",
    "            df = pd.read_csv(file_to_save)\n",
    "    else:\n",
    "    \n",
    "        # ====================== Loading Data from Kaggle ==================================\n",
    "        # You will be using HP's data. Feel free to experiment with other data.\n",
    "        # But while doing so, be careful to have a large enough dataset and also pay attention to the data normalization\n",
    "        df = pd.read_csv(os.path.join('Stocks','hpq.us.txt'),delimiter=',',usecols=['Date','Open','High','Low','Close'])\n",
    "        print('Loaded data from the Kaggle repository')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Loading data from CSV\n",
      "File already exists. Loading data from CSV\n"
     ]
    }
   ],
   "source": [
    "testing_pair = ['AEP', 'CMS']\n",
    "df_1 = get_stock_data('AEP')\n",
    "df_2 = get_stock_data('CMS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "close1 = df_1[\"close\"].tolist()\n",
    "close2 = df_2[\"close\"].tolist()\n",
    "date_all = df_1.date.tolist()\n",
    "spread_close = [s1 - s2 for s1,s2 in zip(close1,close2)]\n",
    "spread_df = pd.DataFrame(spread_close, index = date_all, columns = ['close']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        dataX.append(dataset[i:(i+look_back),0])\n",
    "        dataY.append(dataset[i+look_back,0])\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def LSTM_model(data):\n",
    "\n",
    "    #load data\n",
    "    dataset = np.array(data['close'].values).reshape(-1,1)\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "    # reshape for look_back\n",
    "    look_back = 10\n",
    "    X_train, y_train = create_dataset(train, look_back)\n",
    "    X_test, y_test = create_dataset(test, look_back)\n",
    "\n",
    "    # reshape for LSTM [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # LSTM\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_dim=1)) #look_back))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(X_train, y_train, nb_epoch=100, batch_size=5, verbose=2)\n",
    "\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test) \n",
    "   \n",
    "    # scale back \n",
    "    train_pred = scaler.inverse_transform(train_pred)\n",
    "    y_train = scaler.inverse_transform(y_train)\n",
    "    test_pred = scaler.inverse_transform(test_pred)\n",
    "    y_test = scaler.inverse_transform(y_test)\n",
    "   \n",
    "    # shift predictions for plotting\n",
    "    train_pred_plot = np.empty_like(dataset)\n",
    "    train_pred_plot[:,:] = np.nan\n",
    "    train_pred_plot[look_back:len(train_pred)+look_back,:] = train_pred\n",
    "\n",
    "    test_pred_plot = np.empty_like(dataset)\n",
    "    test_pred_plot[:,:] = np.nan\n",
    "    test_pred_plot[len(train_pred)+(look_back*2)+1:len(dataset)-1,:] = test_pred\n",
    "\n",
    "    f = plt.figure()\n",
    "    plt.plot(scaler.inverse_transform(dataset), color='b', lw=2.0, label='S&P 500')\n",
    "    plt.plot(train_pred_plot, color='g', lw=2.0, label='LSTM train')\n",
    "    plt.plot(test_pred_plot, color='r', lw=2.0, label='LSTM test')\n",
    "    plt.legend(loc=3)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-569fefed240e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLSTM_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspread_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-d2803b939317>\u001b[0m in \u001b[0;36mLSTM_model\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#look_back))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "LSTM_model(spread_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 476.9972891807556,
   "position": {
    "height": "40px",
    "left": "886.1412963867188px",
    "right": "7.771739482879639px",
    "top": "117.9891357421875px",
    "width": "358.2608642578125px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
